{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict if number on image is prime or composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present notebook contains codes for images classification. Modified MNIST dataset has been used. 0 and 1 numbers has been deleted. Labels has been encoded into 0 and 1, where 0 is a prime number (2, 3, 5 or 7) and 1 is a composite number (4, 6, 8, or 9). The model used is custom artificial neural network. Model of the network was created with Tensorflow 2.0 and Keras as high level API. Model was tuned with Scikit-learn library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data exploration\n",
    "2. Data preparation\n",
    "3. Data modelling\n",
    "4. Model assessment\n",
    "5. Hyperparameters tuning\n",
    "6. Model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract specified part of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image data:\n",
    "import gzip\n",
    "# Create BufferedReader object\n",
    "f = gzip.open('train-images-idx3-ubyte.gz','r')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 5\n",
    "\n",
    "import numpy as np\n",
    "# set specificiation (?) of importing bytes\n",
    "f.read(16) # 16\n",
    "\n",
    "# read data as bytes object\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "\n",
    "# Transform bytes to float numbers in 1-dimensional ndarray \n",
    "data_1 = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "# Reshape 1-dimensional array of data to 4 dimensions\n",
    "data = data_1.reshape(num_images, image_size, image_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 1-st row of ndarray (1-st dimension of data)\n",
    "#print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show shape of final ndarray\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show choosen picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Cut first row, and reshape joining all sub arrays (from 1-st row)\n",
    "image = np.asarray(data[0]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[0]\n",
      "[4]\n",
      "[1]\n",
      "[9]\n",
      "[2]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 labels:\n",
    "\n",
    "# Create BufferedReader object\n",
    "f = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "\n",
    "# set specificiation (?) of importing bytes\n",
    "f.read(8) # 8\n",
    "\n",
    "# Print labels in loop\n",
    "for i in range(0,10):   \n",
    "    buf = f.read(1) # read first byte?\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "\n",
    "train_images_arr = idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "# arr is now ndarray object of shape 60000, 28, 28\n",
    "train_labels_arr = idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "\n",
    "test_images_arr = idx2numpy.convert_from_file('t10k-images.idx3-ubyte')\n",
    "test_labels_arr = idx2numpy.convert_from_file('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shape of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Remove 0 and 1 from dataset \n",
    "\n",
    "# 2)  Recode labels from (2,3,5,7) to 0 (as prime numbers)\n",
    "#     Recode labels from (4,6,8,9) to 1 (as composite numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 0 and 1 from train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of 0, 1 from train set / check on which positions they are\n",
    "indices_0_1_train = [i for i, value in np.ndenumerate(train_labels_arr) if value == 0 or value == 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,), (3,), (6,), (8,), (14,), (21,), (23,), (24,), (34,), (37,)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_0_1_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices_0_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check labels in training set\n",
    "train_labels_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of 0, 1 from test set\n",
    "indices_0_1_test = [i for i, value in np.ndenumerate(test_labels_arr) if value == 0 or value == 1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,), (3,), (5,), (10,)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_0_1_test[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete 0 and 1 from images and labels arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete labels of 0, 1 from train set\n",
    "train_labels_arr_2 = np.delete(train_labels_arr, indices_0_1_train, axis=None)\n",
    "# Delete pictures of 0, 1 from train set\n",
    "train_images_arr_2 = np.delete(train_images_arr, indices_0_1_train, axis=0)\n",
    "\n",
    "# Delete labels of 0, 1 from test set\n",
    "test_labels_arr_2 = np.delete(test_labels_arr, indices_0_1_test, axis=None)\n",
    "# Delete pictures of 0, 1 from test set\n",
    "test_images_arr_2 = np.delete(test_images_arr, indices_0_1_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47335,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_arr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47335, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_arr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7885,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_arr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7885, 28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_arr_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)  Recode labels from (2,3,5,7) to 0 (as prime numbers)\n",
    "#     Recode labels from (4,6,8,9) to 1 (as composite numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode labels from (2,3,5,7) to 0 (as prime numbers)\n",
    "train_labels_arr_2[ (train_labels_arr_2 == 2)|(train_labels_arr_2 == 3)|(train_labels_arr_2 == 5)|(train_labels_arr_2 == 7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 9, 0, 0, 4, 0, 0, 0, 6, 0, 0, 8, 6, 9, 4, 9, 0, 4, 0, 0, 0,\n",
       "       0, 8, 6, 9, 0, 6, 0, 6, 8, 0, 9, 0, 9, 8, 0, 9, 0, 0, 0, 4, 9, 8,\n",
       "       9, 4, 4, 4, 6, 4, 0, 6, 0, 6, 0, 0, 0, 9, 0, 6, 0, 8, 0, 9, 4, 6,\n",
       "       0, 4, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_arr_2[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frequencies in changed ndarray\n",
    "(unique, counts) = np.unique(train_labels_arr_2, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 23775],\n",
       "       [    4,  5842],\n",
       "       [    6,  5918],\n",
       "       [    8,  5851],\n",
       "       [    9,  5949]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode labels from (4,6,8,9) to 1 (as composite numbers)\n",
    "train_labels_arr_2[ (train_labels_arr_2 == 4)|(train_labels_arr_2 == 6)|(train_labels_arr_2 == 8)|(train_labels_arr_2 == 9)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frequencies in changed ndarray\n",
    "(unique, counts) = np.unique(train_labels_arr_2, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 23775],\n",
       "       [    1, 23560]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode labels from (2,3,5,7) to 0 (as prime numbers)\n",
    "test_labels_arr_2[ (test_labels_arr_2 == 2)|(test_labels_arr_2 == 3)|(test_labels_arr_2 == 5)|(test_labels_arr_2 == 7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, 4, 9, 0, 9, 6, 9, 0, 9, 0, 0, 4, 9, 6, 6, 0, 4, 0, 4, 0,\n",
       "       0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 4, 6, 0, 0, 0, 6, 4, 9, 0,\n",
       "       0, 8, 9, 0, 0, 4, 6, 4, 0, 0, 0, 9, 0, 0, 0, 9, 0, 0, 6, 0, 0, 8,\n",
       "       4, 0, 0, 6], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_arr_2[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frequencies in changed ndarray\n",
    "(unique, counts) = np.unique(test_labels_arr_2, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 3962],\n",
       "       [   4,  982],\n",
       "       [   6,  958],\n",
       "       [   8,  974],\n",
       "       [   9, 1009]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode labels from (4,6,8,9) to 1 (as composite numbers)\n",
    "test_labels_arr_2[ (test_labels_arr_2 == 4)|(test_labels_arr_2 == 6)|(test_labels_arr_2 == 8)|(test_labels_arr_2 == 9)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frequencies in changed ndarray\n",
    "(unique, counts) = np.unique(test_labels_arr_2, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 3962],\n",
       "       [   1, 3923]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_images_arr_2[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset to <0,1> range\n",
    "train_images_arr_2, test_images_arr_2 = train_images_arr_2 / 255.0, test_images_arr_2 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9921568627450981"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_images_arr_2[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = train_images_arr_2, train_labels_arr_2, test_images_arr_2, test_labels_arr_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far we deleted 0 and 1 from out datasets, both train and test. We also recoded all prime numbers to 0, and all composite numbers to 1. So our classifier will not differentiate between numbers among crtain groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__) # 2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 layers neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "        \n",
    "  # flatten input array to 1d vector\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # output shape is 784\n",
    "  \n",
    "  # Add first layer consisted of 128 neurons, what is resulting \n",
    "  # in 100480 new parameters\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  \n",
    "  # Random dropout of choosen percentage of parameters between layers\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  \n",
    "  # add second layer consisted of 2 neurons\n",
    "  # output shape is 2 - one output value for each prime and composite number\n",
    "  tf.keras.layers.Dense(2, activation='softmax') \n",
    "])\n",
    "# 10 neurons as output in last layer \n",
    "\n",
    "# configure learning process\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show network structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47335/47335 [==============================] - 6s 120us/sample - loss: 0.1169 - accuracy: 0.9583\n",
      "Epoch 2/50\n",
      "47335/47335 [==============================] - 6s 122us/sample - loss: 0.0601 - accuracy: 0.9796\n",
      "Epoch 3/50\n",
      "47335/47335 [==============================] - 7s 140us/sample - loss: 0.0454 - accuracy: 0.9848\n",
      "Epoch 4/50\n",
      "47335/47335 [==============================] - 6s 130us/sample - loss: 0.0358 - accuracy: 0.9876\n",
      "Epoch 5/50\n",
      "47335/47335 [==============================] - 6s 120us/sample - loss: 0.0286 - accuracy: 0.9907\n",
      "Epoch 6/50\n",
      "47335/47335 [==============================] - 5s 109us/sample - loss: 0.0261 - accuracy: 0.9911\n",
      "Epoch 7/50\n",
      "47335/47335 [==============================] - 5s 108us/sample - loss: 0.0226 - accuracy: 0.9924\n",
      "Epoch 8/50\n",
      "47335/47335 [==============================] - 8s 159us/sample - loss: 0.0192 - accuracy: 0.9934\n",
      "Epoch 9/50\n",
      "47335/47335 [==============================] - 8s 163us/sample - loss: 0.0169 - accuracy: 0.9941\n",
      "Epoch 10/50\n",
      "47335/47335 [==============================] - 7s 156us/sample - loss: 0.0156 - accuracy: 0.9944\n",
      "Epoch 11/50\n",
      "47335/47335 [==============================] - 8s 160us/sample - loss: 0.0131 - accuracy: 0.9957\n",
      "Epoch 12/50\n",
      "47335/47335 [==============================] - 8s 163us/sample - loss: 0.0117 - accuracy: 0.9959\n",
      "Epoch 13/50\n",
      "47335/47335 [==============================] - 7s 142us/sample - loss: 0.0121 - accuracy: 0.9958\n",
      "Epoch 14/50\n",
      "47335/47335 [==============================] - 7s 151us/sample - loss: 0.0105 - accuracy: 0.9964\n",
      "Epoch 15/50\n",
      "47335/47335 [==============================] - 7s 139us/sample - loss: 0.0096 - accuracy: 0.9966\n",
      "Epoch 16/50\n",
      "47335/47335 [==============================] - 6s 136us/sample - loss: 0.0103 - accuracy: 0.9965\n",
      "Epoch 17/50\n",
      "47335/47335 [==============================] - 5s 112us/sample - loss: 0.0077 - accuracy: 0.9974\n",
      "Epoch 18/50\n",
      "47335/47335 [==============================] - 6s 120us/sample - loss: 0.0084 - accuracy: 0.9970\n",
      "Epoch 19/50\n",
      "47335/47335 [==============================] - 6s 116us/sample - loss: 0.0075 - accuracy: 0.9976\n",
      "Epoch 20/50\n",
      "47335/47335 [==============================] - 6s 121us/sample - loss: 0.0076 - accuracy: 0.9975\n",
      "Epoch 21/50\n",
      "47335/47335 [==============================] - 6s 123us/sample - loss: 0.0084 - accuracy: 0.9971\n",
      "Epoch 22/50\n",
      "47335/47335 [==============================] - 6s 136us/sample - loss: 0.0069 - accuracy: 0.9978\n",
      "Epoch 23/50\n",
      "47335/47335 [==============================] - 8s 170us/sample - loss: 0.0070 - accuracy: 0.9977\n",
      "Epoch 24/50\n",
      "47335/47335 [==============================] - 8s 163us/sample - loss: 0.0062 - accuracy: 0.9977\n",
      "Epoch 25/50\n",
      "47335/47335 [==============================] - 6s 116us/sample - loss: 0.0061 - accuracy: 0.9981\n",
      "Epoch 26/50\n",
      "47335/47335 [==============================] - 6s 120us/sample - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 27/50\n",
      "47335/47335 [==============================] - 5s 108us/sample - loss: 0.0068 - accuracy: 0.9975\n",
      "Epoch 28/50\n",
      "47335/47335 [==============================] - 6s 121us/sample - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 29/50\n",
      "47335/47335 [==============================] - 6s 127us/sample - loss: 0.0057 - accuracy: 0.9980\n",
      "Epoch 30/50\n",
      "47335/47335 [==============================] - 5s 111us/sample - loss: 0.0062 - accuracy: 0.9979\n",
      "Epoch 31/50\n",
      "47335/47335 [==============================] - 5s 101us/sample - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 32/50\n",
      "47335/47335 [==============================] - 6s 137us/sample - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 33/50\n",
      "47335/47335 [==============================] - 9s 198us/sample - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 34/50\n",
      "47335/47335 [==============================] - 7s 145us/sample - loss: 0.0041 - accuracy: 0.9986\n",
      "Epoch 35/50\n",
      "47335/47335 [==============================] - 6s 132us/sample - loss: 0.0057 - accuracy: 0.9983\n",
      "Epoch 36/50\n",
      "47335/47335 [==============================] - 7s 138us/sample - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 37/50\n",
      "47335/47335 [==============================] - 7s 149us/sample - loss: 0.0044 - accuracy: 0.9986\n",
      "Epoch 38/50\n",
      "47335/47335 [==============================] - 6s 120us/sample - loss: 0.0051 - accuracy: 0.9982\n",
      "Epoch 39/50\n",
      "47335/47335 [==============================] - 6s 123us/sample - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 40/50\n",
      "47335/47335 [==============================] - 6s 129us/sample - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 41/50\n",
      "47335/47335 [==============================] - 6s 119us/sample - loss: 0.0044 - accuracy: 0.9987\n",
      "Epoch 42/50\n",
      "47335/47335 [==============================] - 7s 157us/sample - loss: 0.0036 - accuracy: 0.9989\n",
      "Epoch 43/50\n",
      "47335/47335 [==============================] - 6s 133us/sample - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "47335/47335 [==============================] - 8s 159us/sample - loss: 0.0050 - accuracy: 0.9985\n",
      "Epoch 45/50\n",
      "47335/47335 [==============================] - 9s 195us/sample - loss: 0.0039 - accuracy: 0.9987\n",
      "Epoch 46/50\n",
      "47335/47335 [==============================] - 7s 146us/sample - loss: 0.0042 - accuracy: 0.9987\n",
      "Epoch 47/50\n",
      "47335/47335 [==============================] - 7s 143us/sample - loss: 0.0048 - accuracy: 0.9983\n",
      "Epoch 48/50\n",
      "47335/47335 [==============================] - 6s 123us/sample - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 49/50\n",
      "47335/47335 [==============================] - 6s 117us/sample - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 50/50\n",
      "47335/47335 [==============================] - 7s 142us/sample - loss: 0.0032 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x207cb42eef0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(X_train, Y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7885/7885 [==============================] - 1s 111us/sample - loss: 0.0525 - accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05245432412244046, 0.9907419]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has pretty high 0.9906 accuracy on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network on particular examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get image from test set and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut observation\n",
    "X_obs_02 = np.expand_dims(X_test[0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_test[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000e+00 4.75968e-23]]\n"
     ]
    }
   ],
   "source": [
    "# Predict image class\n",
    "prediction = model.predict(X_obs_02)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablility that number is prime: 1.0000000e+00\n",
    "\n",
    "Probablility that number is composite: 5.532408e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create random image and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereate observation - create an array\n",
    "X_obs_01_2d = np.array([np.round(np.random.uniform(size = None),4) for i in range(784)]).reshape(28,28)\n",
    "# add dim on axis = 0\n",
    "X_obs_01 = np.expand_dims(X_obs_01_2d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_obs_01_2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.250681e-26 1.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Predict image class\n",
    "prediction = model.predict(X_obs_01)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablility that number is prime: 1.9249865e-22 \n",
    "\n",
    "Probablility that number is composite: 1.0000000e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 layers neural network\n",
    "# function should has\n",
    "\n",
    "def create_model(learn_rate = 0.001, momentum = 0.0, activation_layer_2='softmax', activation_layer_1='relu',\n",
    "                neurons_number_layer_1 = 128, dropout_prob_layer_2 = 0.2):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "    # flatten input array to 1d vector\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # output shape is 784\n",
    "  \n",
    "    # Add first layer consisted of neurons\n",
    "    tf.keras.layers.Dense(neurons_number_layer_1, activation=activation_layer_1),\n",
    "  \n",
    "    # Random dropout of choosen percentage of parameters between layers\n",
    "    tf.keras.layers.Dropout(dropout_prob_layer_2),\n",
    "  \n",
    "    # add second layer consisted of 2 neurons\n",
    "    # output shape is 2 - one output value for each prime and composite number\n",
    "    tf.keras.layers.Dense(2, activation=activation_layer_2) \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # configure learning process\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show network structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters' spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below we create lists of parameters, which will be used in random search optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================#\n",
    "# Set parameters beeing passed in .fit method #\n",
    "#=============================================#\n",
    "\n",
    "_batch_size = [20, 60] # [20, 60, 100]\n",
    "_epochs = [10, 20] # [50, 100]\n",
    "\n",
    "#=======================================================#\n",
    "# Set parameters beeing passed to create_model function # (needed to be add as arguments)\n",
    "#=======================================================#\n",
    "\n",
    "_learn_rate = [0.001, 0.3] # [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "_momentum = [0.0] # [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "#_init_mode = ['uniform'] # ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', \n",
    "                         #  'he_uniform']\n",
    "_activation_layer_1 = ['relu', 'tanh'] # ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', \n",
    "                                        # 'linear']\n",
    "_activation_layer_2 = ['softmax']\n",
    "_neurons_number_layer_1 = [128] # [64, 128, 196, 256]\n",
    "#_neurons_number_layer_2 = [40] # [32, 64, 128, 196, 256]\n",
    "#_neurons_number_layer_3 = [10] # [16, 32, 64, 128, 196, 256]\n",
    "_dropout_prob_layer_2 = [0.2]\n",
    "\n",
    "#===========================#\n",
    "# Create dict of parameters #\n",
    "#===========================#\n",
    "\n",
    "param_distributions = dict(batch_size = _batch_size, \n",
    "                           epochs = _epochs,\n",
    "                          \n",
    "                           learn_rate = _learn_rate,\n",
    "                           momentum = _momentum,\n",
    "                           \n",
    "                           #init_mode = _init_mode,\n",
    "                           \n",
    "                           activation_layer_1 = _activation_layer_1, \n",
    "                           activation_layer_2 = _activation_layer_2,\n",
    "                           \n",
    "                           neurons_number_layer_1 = _neurons_number_layer_1,\n",
    "                           # neurons_number_layer_2 = _neurons_number_layer_2,\n",
    "                           # neurons_number_layer_3 = _neurons_number_layer_3\n",
    "                           dropout_prob_layer_2 = _dropout_prob_layer_2,\n",
    "                           )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Wrap model into KerasClassifier to enable passing it to scikit-learn \n",
    "model = KerasClassifier(build_fn=create_model, verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping the model with KerasClassifier enable us to use the model in the way as the native Scikit-learnian model - where we may use Random Search Cross-validation, or make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bartomiej Czajewski\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 16 is smaller than n_iter=60. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed: 15.0min\n",
      "C:\\Users\\Bartomiej Czajewski\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-2)]: Done  48 out of  48 | elapsed: 34.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(14)\n",
    "\n",
    "# Set parameters of cross validation random search\n",
    "rscv = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, \n",
    "                      n_iter=60, cv=3, verbose=4, n_jobs=-2, random_state=2020,\n",
    "                      scoring='accuracy')\n",
    "\n",
    "# Train models\n",
    "rscv_results = rscv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# Show available metrics in sklearn\n",
    "#sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best scores overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMETERS: {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "\n",
      "MEAN ACCURACY SCORE IN CV WITH BEST PARAMETERS: 0.9874300200697158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "# Show best parameters\n",
    "print(\"BEST PARAMETERS: \" + str(rscv_results.best_params_))\n",
    "\n",
    "# Show best CV score\n",
    "print(\"\\nMEAN ACCURACY SCORE IN CV WITH BEST PARAMETERS: \" + str(rscv_results.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full results of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.987430 using {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "\n",
      "Accuracy, standard deviation, rank, parameters :\n",
      "\n",
      "0.985888 0.000233 4 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.985360 0.000155 6 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.987430 0.000244 1 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.985824 0.001001 5 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.984832 0.000375 8 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.984979 0.000804 7 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.986458 0.001449 3 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.986564 0.000315 2 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'relu'}\n",
      "0.982402 0.001335 14 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.983881 0.000577 10 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.984663 0.001186 9 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.983184 0.000617 12 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 20, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.980543 0.001176 16 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.982149 0.001091 15 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 10, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.983754 0.000266 11 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.001, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n",
      "0.982994 0.001978 13 {'neurons_number_layer_1': 128, 'momentum': 0.0, 'learn_rate': 0.3, 'epochs': 20, 'dropout_prob_layer_2': 0.2, 'batch_size': 60, 'activation_layer_2': 'softmax', 'activation_layer_1': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (rscv_results.best_score_, rscv_results.best_params_))\n",
    "means = rscv_results.cv_results_['mean_test_score']\n",
    "stds = rscv_results.cv_results_['std_test_score']\n",
    "params = rscv_results.cv_results_['params']\n",
    "rank = rscv_results.cv_results_['rank_test_score']\n",
    "\n",
    "print(\"\\nAccuracy, standard deviation, rank, parameters :\\n\")\n",
    "for mean, stdev, rank, param in zip(means, stds, rank, params):\n",
    "    print(\"%f %f %d %r\" % (mean, stdev, rank, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy Score - ON TRAIN DATA: 0.9992394633991761\n",
      "\n",
      "Mean Accuracy Score - ON TEST DATA: 0.9893468611287254\n"
     ]
    }
   ],
   "source": [
    "# Predict with best parametrized model on training set\n",
    "#y_pred_train = rs.predict(X_train) # or y_pred_train = rs_results.best_estimator_.predict(X_train)\n",
    "y_pred_cls_02_train = rscv_results.best_estimator_.predict(X_train)\n",
    "\n",
    "# Scoring on train set\n",
    "score_03 = accuracy_score(Y_train, y_pred_cls_02_train)\n",
    "print(\"\\nMean Accuracy Score - ON TRAIN DATA: {}\".format(score_03))\n",
    "\n",
    "# Predict with best parametrized model on test set\n",
    "#y_pred = rs.predict(X_test) # or y_pred = rs_results.best_estimator_.predict(X_test)\n",
    "y_pred_cls_02_test = rscv_results.best_estimator_.predict(X_test)\n",
    "\n",
    "# Scoring on test set\n",
    "score_04 = accuracy_score(Y_test, y_pred_cls_02_test)\n",
    "print(\"\\nMean Accuracy Score - ON TEST DATA: {}\".format(score_04))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
